INFO:__main__:Processed response from llama11b API : The encoders in the picture are:

1. **Multi-Head Attention**
2. **Feed Forward**
3. **Add & Norm**

These are the encoders present in the Encoder block, which is represented on the left side of the image. The Encoder block consists of N times repeated Encoder layers.    

The **Add & Norm** is also known as Layer Normalization and Residual Connection.

There are two **Add & Norm** blocks in the Encoder.

The **Feed Forward** block consists of two linear layers with a ReLU activation function in between.

The **Multi-Head Attention** block allows the model to attend to different parts of the input sequence simultaneously.
INFO:__main__:Processed response from llama90b API : The encoders in this picture are:

*   **Multi-head Attention**
*   **Feed Forward**
*   **Add & Norm**

These encoders are repeated N times.
{'llama11b': 'The encoders in the picture are:\n\n1. **Multi-Head Attention**\n2. **Feed Forward**\n3. **Add & Norm** \n\nThese are the encoders present in the Encoder block, which is represented on the left side of the image. The Encoder block consists of N times repeated Encoder layers. \n\nThe **Add & Norm** is also known as Layer Normalization and Residual Connection. \n\nThere are two **Add & Norm** blocks in the Encoder. \n\nThe **Feed Forward** block consists of two linear layers with a ReLU activation function in between. \n\nThe **Multi-Head Attention** block allows the model to attend to different parts of the input sequence simultaneously.', 'llama90b': 'The encoders in this picture are:\n\n*   **Multi-head Attention**\n*   **Feed Forward** \n*   **Add & Norm** \n\nThese encoders are repeated N times.'}